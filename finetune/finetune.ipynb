{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "43f6afbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f66fd9ff984a8fa435e11ae3c7324e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LoRA configuration set: rank=16, alpha=16, modules=['q_proj', 'k_proj', 'v_proj', 'o_proj'], dropout=0.0\n",
      "Trainer initialized:\n",
      "  Output dir: outputs\n",
      "  Adapter path: adapters\n",
      "  Learning rate: 0.0002\n",
      "  Iterations: 50\n",
      "  Batch size: 2\n",
      "  LoRA r=16, alpha=16\n",
      "  Native training: True\n",
      "  LR scheduler: cosine\n",
      "  Grad checkpoint: False\n",
      "======================================================================\n",
      "Starting Fine-Tuning\n",
      "======================================================================\n",
      "\n",
      "[Using Native MLX Training]\n",
      "\n",
      "Applying LoRA adapters...\n",
      "Applying LoRA to 16 layers: {'rank': 16, 'scale': 1.0, 'dropout': 0.0, 'keys': ['self_attn.q_proj', 'self_attn.k_proj', 'self_attn.v_proj', 'self_attn.o_proj']}\n",
      "✓ LoRA applied successfully to 16 layers\n",
      "  Trainable LoRA parameters: 128\n",
      "Preparing training data...\n",
      "  Detected format: alpaca\n",
      "✓ Prepared 100 training samples\n",
      "  Saved to: outputs/train.jsonl\n",
      "✓ Created validation set (copied from train)\n",
      "\n",
      "Training configuration:\n",
      "  Iterations: 50\n",
      "  Batch size: 2\n",
      "  Learning rate: 0.0002\n",
      "  LR scheduler: cosine\n",
      "  Grad checkpoint: True\n",
      "  Adapter file: adapters/adapters.safetensors\n",
      "\n",
      "Loaded 100 training samples, 100 validation samples\n",
      "Starting training loop...\n",
      "Starting training..., iters: 50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:04<00:00,  5.82it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1: Val loss 1.988, Val took 4.302s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 10: Train loss 1.485, Learning Rate 1.844e-04, It/sec 1.362, Tokens/sec 607.188, Trained Tokens 4457, Peak mem 2.273 GB\n",
      "Iter 20: Train loss 1.327, Learning Rate 1.368e-04, It/sec 3.126, Tokens/sec 811.520, Trained Tokens 7053, Peak mem 2.273 GB\n",
      "Iter 30: Train loss 1.276, Learning Rate 7.513e-05, It/sec 2.256, Tokens/sec 858.579, Trained Tokens 10858, Peak mem 2.273 GB\n",
      "Iter 40: Train loss 1.366, Learning Rate 2.295e-05, It/sec 2.489, Tokens/sec 799.567, Trained Tokens 14070, Peak mem 2.273 GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Calculating loss...: 100%|██████████| 25/25 [00:03<00:00,  6.49it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Val loss 1.179, Val took 3.860s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 50: Train loss 1.273, Learning Rate 1.973e-07, It/sec 1.904, Tokens/sec 720.759, Trained Tokens 17856, Peak mem 2.273 GB\n",
      "Saved final weights to adapters/adapters.safetensors.\n",
      "\n",
      "======================================================================\n",
      "Training Complete!\n",
      "======================================================================\n",
      "  Adapters saved to: adapters\n",
      "Saving LoRA adapters to lora_model...\n",
      "✓ Adapters saved to lora_model\n",
      "Saving merged model to merged...\n",
      "Saving model to merged...\n",
      "✓ Model saved to merged\n",
      "  LoRA adapters will be fused from: adapters\n",
      "Exporting to GGUF format...\n",
      "Exporting model to GGUF format...\n",
      "  Model: mlx-community/Llama-3.2-1B-Instruct-4bit\n",
      "  Output: model/model.gguf\n",
      "  Adapters: adapters\n",
      "\n",
      "Running: mlx_lm.fuse --model mlx-community/Llama-3.2-1B-Instruct-4bit --export-gguf --gguf-path model/model.gguf --adapter-path adapters\n",
      "Error during GGUF export: \n",
      "Fetching 6 files:   0%|          | 0/6 [00:00<?, ?it/s]\n",
      "Fetching 6 files: 100%|██████████| 6/6 [00:00<00:00, 98304.00it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"/Users/mizuy/lab/langmodel/.venv/bin/mlx_lm.fuse\"\u001b[0m, line \u001b[35m10\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    sys.exit(\u001b[31mmain\u001b[0m\u001b[1;31m()\u001b[0m)\n",
      "             \u001b[31m~~~~\u001b[0m\u001b[1;31m^^\u001b[0m\n",
      "  File \u001b[35m\"/Users/mizuy/lab/langmodel/.venv/lib/python3.13/site-packages/mlx_lm/fuse.py\"\u001b[0m, line \u001b[35m64\u001b[0m, in \u001b[35mmain\u001b[0m\n",
      "    model, tokenizer, config = \u001b[31mload\u001b[0m\u001b[1;31m(\u001b[0m\n",
      "                               \u001b[31m~~~~\u001b[0m\u001b[1;31m^\u001b[0m\n",
      "        \u001b[1;31margs.model, adapter_path=args.adapter_path, return_config=True\u001b[0m\n",
      "        \u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "    \u001b[1;31m)\u001b[0m\n",
      "    \u001b[1;31m^\u001b[0m\n",
      "  File \u001b[35m\"/Users/mizuy/lab/langmodel/.venv/lib/python3.13/site-packages/mlx_lm/utils.py\"\u001b[0m, line \u001b[35m324\u001b[0m, in \u001b[35mload\u001b[0m\n",
      "    model = load_adapters(model, adapter_path)\n",
      "  File \u001b[35m\"/Users/mizuy/lab/langmodel/.venv/lib/python3.13/site-packages/mlx_lm/utils.py\"\u001b[0m, line \u001b[35m259\u001b[0m, in \u001b[35mload_adapters\u001b[0m\n",
      "    return _load_adapters(model, adapter_path)\n",
      "  File \u001b[35m\"/Users/mizuy/lab/langmodel/.venv/lib/python3.13/site-packages/mlx_lm/tuner/utils.py\"\u001b[0m, line \u001b[35m127\u001b[0m, in \u001b[35mload_adapters\u001b[0m\n",
      "    with \u001b[31mopen\u001b[0m\u001b[1;31m(adapter_path / \"adapter_config.json\", \"r\")\u001b[0m as fid:\n",
      "         \u001b[31m~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "\u001b[1;35mFileNotFoundError\u001b[0m: \u001b[35m[Errno 2] No such file or directory: 'adapters/adapter_config.json'\u001b[0m\n",
      "\n",
      "\n",
      "⚠️  Model config not found. This usually means:\n",
      "   1. The model path is incorrect\n",
      "   2. The model hasn't been downloaded yet\n",
      "\n",
      "   Try loading the model first with mlx_lm:\n",
      "   python -c \"from mlx_lm import load; load('mlx-community/Llama-3.2-1B-Instruct-4bit')\"\n",
      "\n",
      "Trying alternative export method...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: mlx_lm.convert [-h] [--hf-path HF_PATH] [--mlx-path MLX_PATH] [-q]\n",
      "                      [--q-group-size Q_GROUP_SIZE] [--q-bits Q_BITS]\n",
      "                      [--q-mode {affine,mxfp4}]\n",
      "                      [--quant-predicate {mixed_2_6,mixed_3_4,mixed_3_6,mixed_4_6}]\n",
      "                      [--dtype {float16,bfloat16,float32}]\n",
      "                      [--upload-repo UPLOAD_REPO] [-d] [--trust-remote-code]\n",
      "mlx_lm.convert: error: unrecognized arguments: --export-gguf\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Alternative method also failed: Command '['mlx_lm.convert', '--hf-path', 'mlx-community/Llama-3.2-1B-Instruct-4bit', '-q', '--export-gguf']' returned non-zero exit status 2.\n",
      "\n",
      "Manual export command:\n",
      "  mlx_lm.fuse --model mlx-community/Llama-3.2-1B-Instruct-4bit --export-gguf --gguf-path model/model.gguf\n"
     ]
    },
    {
     "ename": "CalledProcessError",
     "evalue": "Command '['mlx_lm.convert', '--hf-path', 'mlx-community/Llama-3.2-1B-Instruct-4bit', '-q', '--export-gguf']' returned non-zero exit status 2.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lab/langmodel/.venv/lib/python3.13/site-packages/unsloth_mlx/trainer.py:379\u001b[39m, in \u001b[36mexport_to_gguf\u001b[39m\u001b[34m(model_path, output_path, quantization, adapter_path, **kwargs)\u001b[39m\n\u001b[32m    378\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m379\u001b[39m     result = \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcapture_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtext\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    380\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m result.stdout:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.1-macos-aarch64-none/lib/python3.13/subprocess.py:577\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    578\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['mlx_lm.fuse', '--model', 'mlx-community/Llama-3.2-1B-Instruct-4bit', '--export-gguf', '--gguf-path', 'model/model.gguf', '--adapter-path', 'adapters']' returned non-zero exit status 1.",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mCalledProcessError\u001b[39m                        Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 39\u001b[39m\n\u001b[32m     37\u001b[39m model.save_pretrained(\u001b[33m\"\u001b[39m\u001b[33mlora_model\u001b[39m\u001b[33m\"\u001b[39m)  \u001b[38;5;66;03m# Adapters only\u001b[39;00m\n\u001b[32m     38\u001b[39m model.save_pretrained_merged(\u001b[33m\"\u001b[39m\u001b[33mmerged\u001b[39m\u001b[33m\"\u001b[39m, tokenizer)  \u001b[38;5;66;03m# Full model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave_pretrained_gguf\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmodel\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mq4_k_m\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# GGUF\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lab/langmodel/.venv/lib/python3.13/site-packages/unsloth_mlx/model.py:717\u001b[39m, in \u001b[36mMLXModelWrapper.save_pretrained_gguf\u001b[39m\u001b[34m(self, output_dir, tokenizer, quantization_method, **kwargs)\u001b[39m\n\u001b[32m    714\u001b[39m         \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m  Export will use base model only. Train and save adapters first.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    716\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mExporting to GGUF format...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m717\u001b[39m \u001b[43mexport_to_gguf\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    718\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Use original model path, not output directory\u001b[39;49;00m\n\u001b[32m    719\u001b[39m \u001b[43m    \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquantization\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquantization_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    721\u001b[39m \u001b[43m    \u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43madapter_path\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    722\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    723\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/lab/langmodel/.venv/lib/python3.13/site-packages/unsloth_mlx/trainer.py:409\u001b[39m, in \u001b[36mexport_to_gguf\u001b[39m\u001b[34m(model_path, output_path, quantization, adapter_path, **kwargs)\u001b[39m\n\u001b[32m    402\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    403\u001b[39m     alt_cmd = [\n\u001b[32m    404\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmlx_lm.convert\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    405\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m--hf-path\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28mstr\u001b[39m(model_path),\n\u001b[32m    406\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m-q\u001b[39m\u001b[33m\"\u001b[39m,  \u001b[38;5;66;03m# Quantize\u001b[39;00m\n\u001b[32m    407\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m--export-gguf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    408\u001b[39m     ]\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43msubprocess\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43malt_cmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m✓ Model exported using alternative method\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    411\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(output_path)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.local/share/uv/python/cpython-3.13.1-macos-aarch64-none/lib/python3.13/subprocess.py:577\u001b[39m, in \u001b[36mrun\u001b[39m\u001b[34m(input, capture_output, timeout, check, *popenargs, **kwargs)\u001b[39m\n\u001b[32m    575\u001b[39m     retcode = process.poll()\n\u001b[32m    576\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m check \u001b[38;5;129;01mand\u001b[39;00m retcode:\n\u001b[32m--> \u001b[39m\u001b[32m577\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m CalledProcessError(retcode, process.args,\n\u001b[32m    578\u001b[39m                                  output=stdout, stderr=stderr)\n\u001b[32m    579\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m CompletedProcess(process.args, retcode, stdout, stderr)\n",
      "\u001b[31mCalledProcessError\u001b[39m: Command '['mlx_lm.convert', '--hf-path', 'mlx-community/Llama-3.2-1B-Instruct-4bit', '-q', '--export-gguf']' returned non-zero exit status 2."
     ]
    }
   ],
   "source": [
    "from unsloth_mlx import FastLanguageModel, SFTTrainer, SFTConfig\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Load any HuggingFace model (1B model for quick start)\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name=\"mlx-community/Llama-3.2-1B-Instruct-4bit\",\n",
    "    max_seq_length=2048,\n",
    "    load_in_4bit=True,\n",
    ")\n",
    "\n",
    "# Add LoRA adapters\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r=16,\n",
    "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    "    lora_alpha=16,\n",
    ")\n",
    "\n",
    "# Load a dataset (or create your own)\n",
    "dataset = load_dataset(\"yahma/alpaca-cleaned\", split=\"train[:100]\")\n",
    "\n",
    "# Train with SFTTrainer (same API as TRL!)\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    train_dataset=dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    args=SFTConfig(\n",
    "        output_dir=\"outputs\",\n",
    "        per_device_train_batch_size=2,\n",
    "        learning_rate=2e-4,\n",
    "        max_steps=50,\n",
    "    ),\n",
    ")\n",
    "trainer.train()\n",
    "\n",
    "# Save (same API as Unsloth!)\n",
    "model.save_pretrained(\"lora_model\")  # Adapters only\n",
    "model.save_pretrained_merged(\"merged\", tokenizer)  # Full model\n",
    "model.save_pretrained_gguf(\"model\", tokenizer, quantization_method=\"q4_k_m\")  # GGUF\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "85d1a46c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['output', 'input', 'instruction'],\n",
       "    num_rows: 100\n",
       "})"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f847b210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<unsloth_mlx.model.MLXModelWrapper at 0x13b34d940>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
